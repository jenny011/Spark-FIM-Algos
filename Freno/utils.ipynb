{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import RDD, SparkConf, SparkContext\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tree import Tree, TreeNode\n",
    "import threading\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "os.environ[\"PYTHONHASHSEED\"]=str(232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanDB(path, seperation):\n",
    "    db = []\n",
    "    f = open(path, 'r')\n",
    "    for line in f:\n",
    "        if line:\n",
    "            temp_list = line.rstrip().split(seperation)\n",
    "            temp_list = [int(i) for i in temp_list]\n",
    "            temp_list.sort()\n",
    "            temp_list = [str(i) for i in temp_list]\n",
    "            db.append(temp_list)\n",
    "    f.close()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFreno(transactions, minsup):\n",
    "    # for each worker: input (transactions line, minsup) and return minsup list\n",
    "    \n",
    "    tree = Tree(minsup)\n",
    "    for trx in transactions:\n",
    "        tree.insert(tree._root,trx)\n",
    "    return tree\n",
    "    \n",
    "def concatFreno(transactions, tree):\n",
    "    # for each worker: input (transactions line, minsup) and return minsup list\n",
    "    \n",
    "    for trx in transactions:\n",
    "        tree.insert(tree._root,trx)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incFreno(incDBPath, minsup, sc, k, freqRange, res):\n",
    "    \n",
    "    deltaDBRaw = scanDB(incDBPath, \" \")\n",
    "    \n",
    "    \n",
    "    out_rdd = []\n",
    "    for trx in deltaDBRaw:\n",
    "        out_rdd.extend([trx[i:] for i in range(len(trx))])\n",
    "    transDataFile = sc.parallelize(out_rdd)\n",
    "    transData = transDataFile.map(lambda v: (v[0], v))\n",
    "    transData = transData.map(lambda v: v[1])\n",
    "    transData = transData.groupBy(lambda v: int(v[0])%k).map(lambda v : (v[0], list(v[1]))).collect()\n",
    "    \n",
    "    \n",
    "    res = freqRange.map(lambda v: concatFreno(transData[v][1],res[v])).collect()\n",
    "    return freqRange, res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distFreno(inFile, minsup, sc, k, freqRange):\n",
    "    \n",
    "    transDataRaw = scanDB(inFile, \" \")\n",
    "    #print(\"minsup\", minsup)\n",
    "    \n",
    "    out_rdd = []\n",
    "    for trx in transDataRaw:\n",
    "        out_rdd.extend([trx[i:] for i in range(len(trx))])\n",
    "    #print(out_rdd[:5])\n",
    "    \n",
    "    transDataFile = sc.parallelize(out_rdd)\n",
    "    #print(transDataFile.count())\n",
    "    \n",
    "    transData = transDataFile.map(lambda v: (v[0], v))\n",
    "    #print(transData.keys().take(5))\n",
    "    transData = transData.map(lambda v: v[1])\n",
    "    \n",
    "    transData = transData.groupBy(lambda v: int(v[0])%k).map(lambda v : (v[0], list(v[1]))).collect()#.sortByKey()\n",
    "    print(transData[0][1][:5])\n",
    "    \n",
    "    \n",
    "    #print(\"transaction data num of keys:\", transData.count())\n",
    "\n",
    "    #transDataList = transData.collect()\n",
    "    #print(transDataList[0])\n",
    "    \n",
    "    # use the configuration as the number of partitions\n",
    "    print(\"number of partitions used: {}\".format(sc.defaultParallelism))\n",
    "    # print(itemTidsParts.take(5))\n",
    "\n",
    "    #phase 3: Freno from k-itemsets\n",
    "    #freqRange = sc.parallelize(range(0, k))\n",
    "    #freqItemsListToRun = freqRange.map(\\\n",
    "    #    lambda v: transData[v])\n",
    "\n",
    "    #print('freqItemsListToRun', freqItemsListToRun.take(1)[0])\n",
    "    \n",
    "    res = freqRange.map(lambda v: runFreno(transData[v][1],minsup)).collect()\n",
    "    return freqRange, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30'], ['16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30'], ['24', '25', '26', '27', '28', '29', '30'], ['32', '33'], ['40', '41', '42', '43', '44', '45', '46', '47']]\n",
      "number of partitions used: 8\n",
      "[['40'], ['49'], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "testFiles = [\"retail\"]\n",
    "support = [0.4]\n",
    "partition = 8\n",
    "interval = [20000]\n",
    "\n",
    "conf = SparkConf().setAppName(\"\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "schema = StructType([\n",
    "    StructField(\"algorithm\", StringType(), False),\n",
    "    StructField(\"datasets\", StringType(), False),\n",
    "    StructField(\"support\", FloatType(), False)\n",
    "])\n",
    "for i in range(1):\n",
    "    schema.add(\"test{}\".format(i+1), FloatType(), True)\n",
    "#experiments = []\n",
    "\n",
    "for f in testFiles:\n",
    "    for s in support:\n",
    "        for i in interval:\n",
    "            for t in range(1):\n",
    "                transDataRaw = scanDB(\"./datasets/{}.txt\".format(f), \" \")\n",
    "                numTrans = len(transDataRaw)\n",
    "                minsup = s * numTrans\n",
    "\n",
    "                incDir = \"./incdatasets/interval_{0}_{1}\".format(f,i)\n",
    "                incNames = os.listdir(incDir)\n",
    "\n",
    "                freqRange = sc.parallelize(range(0, partition))\n",
    "                freqRange, res = distFreno(os.path.join(incDir,\"db_0.txt\"), minsup, sc, partition, freqRange)\n",
    "                \n",
    "                for incName in incNames[1:]:\n",
    "                    freqRange, res = incFreno(os.path.join(incDir,incName), minsup, sc, partition, freqRange, res)\n",
    "                print(res)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
