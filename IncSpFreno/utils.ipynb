{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import RDD, SparkConf, SparkContext\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tree import Tree, TreeNode\n",
    "import threading\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "os.environ[\"PYTHONHASHSEED\"]=str(232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanDB(path, seperation):\n",
    "    db = []\n",
    "    f = open(path, 'r')\n",
    "    for line in f:\n",
    "        if line:\n",
    "            temp_list = line.rstrip().split(seperation)\n",
    "            temp_list = [int(i) for i in temp_list]\n",
    "            temp_list.sort()\n",
    "            temp_list = [str(i) for i in temp_list]\n",
    "            db.append(temp_list)\n",
    "    f.close()\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFreno(transactions, minsup):\n",
    "    # for each worker: input (transactions line, minsup) and return minsup list\n",
    "    \n",
    "    tree = Tree(minsup)\n",
    "    for trx in transactions:\n",
    "        tree.insert(tree._root,trx)\n",
    "    return tree\n",
    "    \n",
    "def concatFreno(transactions, tree):\n",
    "    # for each worker: input (transactions line, minsup) and return minsup list\n",
    "    \n",
    "    for trx in transactions:\n",
    "        tree.insert(tree._root,trx)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incFreno(incDBPath, minsup, sc, k, freqRange, res):\n",
    "    \n",
    "    deltaDBRaw = scanDB(incDBPath, \" \")\n",
    "        \n",
    "    out_rdd = []\n",
    "    for trx in deltaDBRaw:\n",
    "        out_rdd.extend([trx[i:] for i in range(len(trx))])\n",
    "    transDataFile = sc.parallelize(out_rdd)\n",
    "    transData = transDataFile.map(lambda v: (v[0], v))\n",
    "    transData = transData.map(lambda v: v[1])\n",
    "    transData = transData.groupBy(lambda v: int(v[0])%k).map(lambda v : (v[0], list(v[1]))).collect()\n",
    "    \n",
    "    \n",
    "    res = freqRange.map(lambda v: concatFreno(transData[v][1],res[v])).collect()\n",
    "    return freqRange, res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distFreno(inFile, minsup, sc, k, freqRange):\n",
    "    \n",
    "    transDataRaw = scanDB(inFile, \" \")\n",
    "    \n",
    "    out_rdd = []\n",
    "    for trx in transDataRaw:\n",
    "        out_rdd.extend([trx[i:] for i in range(len(trx))])\n",
    "    transDataFile = sc.parallelize(out_rdd)    \n",
    "    transData = transDataFile.map(lambda v: (v[0], v))\n",
    "    transData = transData.map(lambda v: v[1])\n",
    "    \n",
    "    transData = transData.groupBy(lambda v: int(v[0])%k).map(lambda v : (v[0], list(v[1]))).collect()#.sortByKey()\n",
    "\n",
    "    print(\"number of partitions used: {}\".format(sc.defaultParallelism))\n",
    "\n",
    "    #phase 3: Freno from k-itemsets\n",
    "    res = freqRange.map(lambda v: runFreno(transData[v][1],minsup)).collect()\n",
    "    return freqRange, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions used: 8\n",
      "[['40'], ['49'], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "testFiles = [\"retail\"]\n",
    "support = [0.4]\n",
    "partition = 8\n",
    "interval = [20000]\n",
    "\n",
    "conf = SparkConf().setAppName(\"\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "schema = StructType([\n",
    "    StructField(\"algorithm\", StringType(), False),\n",
    "    StructField(\"datasets\", StringType(), False),\n",
    "    StructField(\"support\", FloatType(), False)\n",
    "])\n",
    "for i in range(1):\n",
    "    schema.add(\"test{}\".format(i+1), FloatType(), True)\n",
    "#experiments = []\n",
    "\n",
    "for f in testFiles:\n",
    "    for s in support:\n",
    "        for i in interval:\n",
    "            for t in range(1):\n",
    "                transDataRaw = scanDB(\"./datasets/{}.txt\".format(f), \" \")\n",
    "                numTrans = len(transDataRaw)\n",
    "                minsup = s * numTrans\n",
    "\n",
    "                incDir = \"./incdatasets/interval_{0}_{1}\".format(f,i)\n",
    "                incNames = os.listdir(incDir)\n",
    "\n",
    "                freqRange = sc.parallelize(range(0, partition))\n",
    "                freqRange, res = distFreno(os.path.join(incDir,\"db_0.txt\"), minsup, sc, partition, freqRange)\n",
    "                \n",
    "                for incName in incNames[1:]:\n",
    "                    freqRange, res = incFreno(os.path.join(incDir,incName), minsup, sc, partition, freqRange, res)\n",
    "                print(res)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
